# Altmetricon #
*London*

## Wednesday, 9 September 2015##

### Welcome
*Euan Adie, Founder*

What we’ve learned from trying to measure impact

+ You probably can’t measure impact
+ Alt metrics = a bad name; not alternative and not really ‘measuring’ or judging quantitatively

Easy for people to conflate different aspects:

+ Quality
+ Attention
+ Impact

+ Engaging with a specific audience --> attention leads to impact
+ AltMetric can help with these.  Quality is another realm of expterise.

### Metrics in Research Evaluation
*Jane Tinkler, Senior Advisor, Parliamentary Office of Science and Technology*
@janetinkler

Discussing large-scale consultation and research done with HEFCE / LSE regarding metrics (www.responsiblemetrics.org)

+ Particularly notes literature review and correlation report

Findings:

+ Contested across the sector (majority of 153 responses ‘sceptical’ about the use of metrics for measuring (quality of) research)
+ Academics felt they were an imposed black box, the gathering methodology unknown to them
+ Peer review, despite limitations, had widespread support, particularly in Arts and Humanities

**“Inappropriate indicators create perverse incentives”.**

+Legitimate questions about gaming the system, self-citation and disadvantaged groups (ECRs)

**“Responsible Metrics”**

+ Robustness
+ Humility
+ Transparency
+ Diversity
+ Reflectivity

**Recommendations**

+ HEI leaders should develop a clear statement of principles on their approach to research management and assessment
  + Only 3 HEIs have signed up to DORA (Declaration on Research Assessment)
  + Promotion and Hiring Panels need explicit declarations of how such metrics are used
+ Research managers should champion these principles and show how metrics will be used
+ Researchers should be mindful of limitations of particular indicators in their CVs and in evaluations of other colleague’s work

Notetaker: Number #3 very important and so easy!

+ Data Providers and rankers (league tales) should strive for greater transparency and interoperability between 
  + HEIs / Researchers also need to be mindful of hypocrisy year to year approval of certain tables
+ Publishers should reduce emphasis on journal impact factors as a promotional tool and use them only in the context of a variety of metrics that provide a richer view of performance

**Infrastructure**

Indicators can only meet their potential if they are underpinned by a robust interoperate infrastructure

+ Use Orcid (mandatory for REF?)
+ Use DOIs for all research outputs.  

Notetaker: Rio Journal and Figshare help

The community needs a mechanism to carry forward this agenda.  Proposes a Forum for Responsible Metrics.

Also an annual ‘Bad Metric’ award (please let responsiblemetrics.org if you see any really bad examples!)
____
#### Nota Bene

***This work is licensed under a Creative Commons Attribution 3.0 Unported License.***

<a rel="license" href="http://creativecommons.org/licenses/by/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by/3.0/88x31.png" /></a>

***Taken Live and should not be considered a definitive or complete record
