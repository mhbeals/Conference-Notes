# Altmetricon #
*London*

## Wednesday, 9 September 2015##

### Welcome
*Euan Adie, Founder*

What we’ve learned from trying to measure impact

+ You probably can’t measure impact
+ Alt metrics = a bad name; not alternative and not really ‘measuring’ or judging quantitatively

Easy for people to conflate different aspects:

+ Quality
+ Attention
+ Impact

+ Engaging with a specific audience --> attention leads to impact
+ AltMetric can help with these.  Quality is another realm of expterise.

### Metrics in Research Evaluation
*Jane Tinkler, Senior Advisor, Parliamentary Office of Science and Technology*
@janetinkler

Discussing large-scale consultation and research done with HEFCE / LSE regarding metrics (www.responsiblemetrics.org)

+ Particularly notes literature review and correlation report

Findings:

+ Contested across the sector (majority of 153 responses ‘sceptical’ about the use of metrics for measuring (quality of) research)
+ Academics felt they were an imposed black box, the gathering methodology unknown to them
+ Peer review, despite limitations, had widespread support, particularly in Arts and Humanities

**”Inappropriate indicators create perverse incentives”.**

+Legitimate questions about gaming the system, self-citation and disadvantaged groups (ECRs)

**”Responsible Metrics”**

+ Robustness
+ Humility
+ Transparency
+ Diversity
+ Reflectivity

**Recommendations**

+ HEI leaders should develop a clear statement of principles on their approach to research management and assessment
  + Only 3 HEIs have signed up to DORA (Declaration on Research Assessment)
  + Promotion and Hiring Panels need explicit declarations of how such metrics are used
+ Research managers should champion these principles and show how metrics will be used
+ Researchers should be mindful of limitations of particular indicators in their CVs and in evaluations of other colleague’s work

Notetaker: Number #3 very important and so easy!

+ Data Providers and rankers (league tales) should strive for greater transparency and interoperability between 
  + HEIs / Researchers also need to be mindful of hypocrisy year to year approval of certain tables
+ Publishers should reduce emphasis on journal impact factors as a promotional tool and use them only in the context of a variety of metrics that provide a richer view of performance

**Infrastructure**

Indicators can only meet their potential if they are underpinned by a robust interoperate infrastructure

+ Use Orcid (mandatory for REF?)
+ Use DOIs for all research outputs.  

Notetaker: Rio Journal and Figshare help

The community needs a mechanism to carry forward this agenda.  Proposes a Forum for Responsible Metrics.

Also an annual ‘Bad Metric’ award (please let responsiblemetrics.org if you see any really bad examples!)

### Funder (Really Wellcome Trust, speaker admits) Approaches to Altmetrics
*Adam Dinsmore, Evaluation Officer, Wellcome Trust*

Current Grant Portfolio Largely Science, also Medical Humanities and Technology

**Varying Audiences**

+ Visitors to gallery
+ School children
+ Readers of publications
+ Policy documents
+ Metrics

**Key Indicators**

+ Discoveries
+ Applications
+ Engagement
+ Research Leaders
+ Research Environment
+ Influence

Altmetrics will help with these

**Our Altmetric Explorations so far...*

+ We will probably NEVER use altmetrics to decide funding outcomes

Case Studies

+ Measuring Twitter mentions of Wellcome-Trust associated articles
  + Showed an insight into the impressive influence of an article that ‘appeared parochial’ long before it was ever formally cited
+ Benchmarking between funders (blogs, Wikipedia, twitter, facebook, etc)
+ Better understanding of scholarly communication. Medium to large correlations (as represented by google scholar) between Mendeley readership and citations
+ Updating PubMed1000 using Altmetric data to give a richer view

Two recurrent Observations

+ Links between grants and citeable items still difficult to establish
+ ‘Indicators can only meet their potential’ if in interoperable infrastructure

WellcomeTrust now mandates (not ABSOLUTELY mandatory...) ORCID ideas to link grants and outputs for analysis

Helps avoid ‘Wellcome Trust did not fund this study’ acknowledgements (snickers from audience)

### Researcher Viewpoint
*Jon Tennant, PhD Imperial College London*
@Protohedgehog

Favourite metric: Kardashian Index

Altmetrics a highly variable basket of metrics. 

Four different types of Altmetrics (Roughly)

+ Scholarly activity (Mendeley)
+ Social Activity 
+ Scholarly Commentary (Blogs, Wikipedia)
+ Mass media Coverage 

**Most publishers now employ some sort of altmetric measuring system**

Notetaker: Love the ‘ability to do sciencey things’ graph

**Scope**

+ Development of persona for researcher
+ Measures social influences (some aspects)
  + Helps researchers decided which platforms they want to engage in
+ Recognises need for public disseminations

Scale of social forums in inspiring (100,000,000 possible audience!)

**Altmetrics as a Part of Open Science**

Altmetrics are Essential Tools for ECRS

+ Blogging, tweeting, etc
+ How do you measure the success of these?
+ Part of the ‘open science’ toolkit for researchers
+ Goes beyond traditional measures of impact
+ An integral part of 21st Century science

“Shows you are thinking beyond the academy”

Discussing impact factor disease -- name and shame those who use it?

Citation cycle (even in STEM) is not as fast as we need it for job interviews (so true!) so altmetrics can provide a suggestion and proxy for later influence

*Final thoughts*

+ Everyone is sceptical
+ Metrics can’t replace peer-reviews
+ Assessment requires a combination of qualitative and carefully selected quantitative indictors
+ Metrics can be used to compliment narrative descriptions of influences
(partial list, slow fingers)

*The Science?*

+ Positive correlation between rapid citations and twitter mentions
+ Highly tweeted articles 11x more likely to be cited
+ Positive but weak correlations between altmetrics and citations
+ Different ‘flavours’ of ‘impact’ that capture different aspects of tools and audiences

**Altmetrics measuring quality?**

+ What is high quality research?
+ Journal and publisher brands, citations, impact factors, peer review procedures?
+ How do you measure inspiration? Education? Creativity? Innovation?
+ How do Altmetrics fit into this?

**Discussion**

+ Altmetrics don’t solve our problems about assessing impact of research
+ But they do help us think more about its societal reach
+ Have to use metrics responsibly
+ Understand realities of academia
+ What can ECRs do?
  + Sign DORA!
  + Discourage use of impact factors
  + Encourage use of altmetrics and social media
  + Influence performance metrics at an institutional level

### Implementing Altmetric for our Institution
*Liam Cleere, Senior Manager, Research Reporting and Analytics, University College Dublin*

**What is impact?**

“The consequences of an action that affect people’s lives in areas that matter to them”

**Altmetric for institutions**

Goals:

+ Which journals achieve societal impact
+ Sharpen research communication and promotion strategy
+ Help researchers find evidence of societal impact from research
+ Analyse trusted data to help articular societal impact to funders

**Bottom up --> Communications Supports**

+ Provide practical support to leverage social media
  + Automated twitter feed
    + Collection from Research Management System -> RSS -> Twitterfeed to Twitter/Linkedin/Facebook (targeted by school)  Measure with Altmetric
+ Research methods and impact activities
  + Review of international best practice
  + On going review of Altmetric tool
    + High Impact Journals high score on STEM Altmetric
    + Poor coverage of A&H because we don’t have as many DOIs / output
    + Twitter and Mendeley best appearance
    + Highest Altmetric was Arts – Article on Neanderthals (Celtic studies)
    + High correlation between citations and Altmetric activity per channel: Mendeley (81%) and Twitter highest (90%) frequency in highly cited articles, but Mendeley and CiteULike best correlation with citations
  + Analysis of communications training at university
    + College specific brochures to help and streamline this work with researchers

**Top Down --> Excellence and Impact Framework**

+ Development of Key Performance Indicators
  + Funding
  + People
  + Dissemination
  + Impact
      + Citations
      + Altmetric “Mentions”
		+ Aggregate by schools and college and compare to other institutions

**Results**

+ Correlative relationship between citations and mentions
+ Provide researchers with evidence of the societal impact
+ Sharpen communications strategy
+ Policy documents as indicators of broader impact
+ Monitoring progress against a comparator group of institutions

Question from audience: What are communicating to researchers about where to publish

Answer: We aren’t. We are helping with quantity and discoverability, but academic freedom is important to us.

Notetaker: Good :)

____
#### Nota Bene

***This work is licensed under a Creative Commons Attribution 3.0 Unported License.***

<a rel="license" href="http://creativecommons.org/licenses/by/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by/3.0/88x31.png" /></a>

***Taken Live and should not be considered a definitive or complete record

